# ТЗ: Специализированный сервис для извлечения метаданных из DDL скриптов

## Проблема

При работе с большим количеством DDL скриптов MSSQL возникает задача автоматического извлечения метаданных о создаваемых объектах базы данных (таблицы, процедуры, функции, представления и т.д.).

### Текущая ситуация

Используется утилита `mentator-promter`, которая отправляет DDL скрипты различным LLM моделям через Ollama API с требованием вернуть JSON с метаданными.

### Проблемы текущего подхода

1. **Нестабильность формата ответа**
   - LLM модели часто возвращают текстовые описания вместо JSON
   - Даже с использованием параметра `format` (JSON Schema) модели игнорируют требования на сложных DDL скриптах
   - Для простых файлов (CREATE TABLE, 10-20 строк) JSON получается корректный
   - Для сложных файлов (CREATE PROCEDURE, 400+ строк) модели выдают текстовое описание функциональности

2. **Ограниченный контроль**
   - Ollama не предоставляет прямой доступ к GBNF грамматикам
   - Невозможно тонко настроить поведение constraint generation
   - Нет возможности диагностировать почему модель не следует формату

3. **Универсальность vs Специализация**
   - Ollama - универсальный инструмент для различных задач (чаты, embeddings, генерация)
   - Для нашей задачи нужна только одна функция: DDL код → структурированный JSON
   - Вся остальная функциональность Ollama избыточна

## Предлагаемое решение

Разработать специализированный HTTP сервис для извлечения метаданных из DDL скриптов с гарантированным возвратом JSON.

### Ключевые принципы

1. **Узкая специализация**
   - Один запрос = один JSON ответ (массив)
   - Нет чатов, истории, контекста - каждый запрос независим

2. **Гарантированный формат**
   - Ответ ВСЕГДА в формате JSON (массив)
   - Если ответ пустой → пустой массив
   - Если произошла ошибка → JSON с описанием ошибки
   - Никогда не возвращается текстовое описание

3. **Гибкость вопроса** В каждом вопросе
   - Модель
   - System promt
   - User promt
   - Нужная в ответе схема JSON (всегда описыватся один элемент массива, т.к. в ответе всегда массив, даже если нужен один элемент)
   - Настройка параметров генерации (temperature, max tokens)

4. **Прозрачность работы**
   - Возможность диагностировать почему модель не справилась

## Функциональные требования

 - node (bun)
 - fastify+swagger
 - node-llama-cpp

### API

 - GET /stat
 Страница с базойо статитстикой работы сервиса (сколько времени работает, какие модели есть, какая модель загружена и т.д.)
 - GET /chat
 Простой чат с самим же сервисом
 - GET /doc
 SWAGGER документация
 - POST /promt
 основной запрос
 {
      model: string,
      message: {
         system?: string,
         user: string
      },
      durationMsec: number,
      options?: {
         temperature?: number,
         ...
      },
      format: {
         в каком формате? вроде бы есть нечто типа Zod для этого
      }
 }
 ответ (если с кодом 200)
 {
   durationMsec: number,
	result: {
		loadModelStatus: 'load' | 'exists'
		data: вот тут результат ответа модели в виде JSON
   },
 }

## Ожидаемые преимущества

1. **Надежность**
   - Гарантированный формат ответа
   - Нет случайных текстовых описаний

2. **Производительность**
   - Модель загружается один раз при старте сервиса
   - Нет overhead на управление чатами и историей
   - Оптимизация под конкретную задачу

3. **Контроль**
   - Полный доступ к GBNF грамматикам
   - Возможность экспериментировать с разными подходами
   - Детальная диагностика проблем

4. **Гибкость**
   - Настройка схемы вывода под разные задачи
   - Возможность A/B тестирования подходов

