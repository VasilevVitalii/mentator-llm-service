1. Сделать docker-контейнер
* предусмотреть папку для монтирования на volume, ее состав:
 * /mentator-llm-service.db
 * /mentator-llm-service.conf.jsonc
 * /gguf/
* в контейнере должно быть установлено все что нужно для работы node-llama-cpp (как для nvidia, так и для amd)
* контейнер должен ументь работать с GPU
* написать build-скрипт (на js), расположить его в папке .auto, который
при запуске создает такой контейнер, этот скрипт указать в project.json
* точка запуска - src/index.ts
* должна быть в контейнере доступна одна модель, сейчас она лежит тут - /home/vitalii/GGUF/qwen2.5-0.5b-instruct-q8_0.gguf

2. Подумать - а может как-то можно сделать автоскачивание gguf? Как-то так - в той папке, где хранятся gguf-ы, организовать папку "_download". Эту папку исключить из сканирования папок в механизме получения списка всех gguf-ов. В эту папку делать докачку модели, потом, когда докачается - сделать из скачанных кусков gguf и перенести на уровень выше - к остальным gguf-ам. Тут только непонятно как пользователю указывать имя модели - вручную (тогда он должен название получить где-то извне) или показать список (откуда его взять?). Ну и сайт нужно выбрать - откуда все это можно качать.
3. Если уже в системе установлен докер с Ollama, то попробовать подключать ее скачанные модели.
