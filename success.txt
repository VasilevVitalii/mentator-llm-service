Задача:

1. Подать в LLM DDL-скрипт и попросить вернуть JSON с описанием того, что там находится, что-то типа такого:
[
  {
    "object_kind": "PROCEDURE",
    "object_name": "MyProcName",
    "database_name": "MyDbName",
    "schema_name": "MySchema",
    "line_start": 3,
    "line_stop": 469
  }
]
По сути аналог ANTLR.
2. При этом мы знаем только вендора БД (Oracle, MSSQL, ext), но не знаем, сколько сущностей (и какие) в конкретном файле.
3. Решение должно уметь обрабатывать файлы размером до 50Кб текста.
4. Решение должно работать на компе уровня "добротная рабочая станция" - например как у меня - проц Intel Core i7 14700KF, GPU Palit NVIDIA GeForce RTX 4060TI RTX4060Ti
JETSTREAM 16ГБ.

Попытка решения:

1. Запустить локально ollama.
2. Подключить подходящую модель.
3. Подобрать промт.
4. Получить JSON.

Проблема:

На файлах размером > 20 Кб текста - не работает.
Перепробовал кучу моделей (штук 20, некоторые размером 30b), кучу промтов (штук 500), различные варианты настроек ("temperature","top_k","top_p","num_ctx").
В ЛУЧШЕМ случае - текст + JSON неправильного формата (и то - результат нестабильный).
При этом по тексту видно, что модель видит в файле все что нужно, но вот выдать нужный JSON - фигушки.

Решение:

Оказалось что проблема в том, что почти все (или вообще все) модели по "умолчанию" заточены на вот такие ответы-тексты/рассуждения, а не формализованный вывод.
И чем слабее комп (и соответственно модели) - тем сильнее это проявляется. На "больших" мощностях и моделях такую проблему можно побороть и классическим подходом (и то без 100% гарантии).
А чтобы это побороть на "маленьких" мощностях, нужно настраивать грамматики у самой модели (GBNF).
Но ollama не позволяет это делать.
Вроде бы есть форк ollama, который умеет настраивать GBNF, но я решил сделать свой узкоспециализированный сервис по выдаче именно формализованных ответов.
Идея простая - "никакой предыстории нет, вот тебе данные, обработай их таким-то способом и дай формализованный ответ".
Там много в итоге заморочек и сейчас сервис еще не доделан (основа работает, но сервис не в prodaction виде - нет логирования и прочих дополнительных красивостей).
НО! core-возможность (REST-API с пока одним методом) РАБОТАЕТ!!! Разные промты - 100% выдают правильный JSON!!! Проверяю на модели Qwen2.5-Coder-7B-Instruct-Q5_K_M.0.gguf.
Как доделаю - будет беслатно тут - https://github.com/VasilevVitalii/mentator-llm-service (пока там сервис-пустышка)

Кто молодец? - Я молодец! )))))))

К размышлению:

1. Применение этого сервиса не ограничивается парсингом DDL-файлов. По сути это обработчик любых текстовых данных с результатом гарантированного формата.
2. В тех текстах, что я изучал по AI, упоминаний GBNF очень мало. Просто интересно - это неооценная или редконужная возможность LLM...

